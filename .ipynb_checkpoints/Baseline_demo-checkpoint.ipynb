{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines on the Synthetic and Real Datasets\n",
    "### This Jupyter Notebook simulates 9 baseline methods on the synthetic and real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn import linear_model, metrics, neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultiflow.trees import HoeffdingAdaptiveTreeClassifier\n",
    "from skmultiflow.meta import AdaptiveRandomForestClassifier\n",
    "from skmultiflow.meta import LearnPPNSEClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from model import NormalNN, NNClassifier\n",
    "\n",
    "from utils import prepare_data\n",
    "from DSS.dataloader.glisterdataloader import GLISTERDataLoader\n",
    "from DSS.dataloader.gradmatchdataloader import GradMatchDataLoader\n",
    "from DSS.simpleNN_net import TwoLayerNet\n",
    "from dotmap import DotMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "if cuda:\n",
    "    device = 'cuda:6'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## na√Øve: Full Data and Current Seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:  SEA\n",
      "concept drifts:  [ 2000  4000  6000  8000 10000 12000 14000 16000]\n",
      "----------------------------------------------------------------------------\n",
      "method:  Full data\n",
      "overall train time: 3.417\n",
      "overall test acc: avg 0.849, std 0.005\n",
      "overall test f1: avg 0.881, std 0.004\n",
      "----------------------------------------------------------------------------\n",
      "method:  Current seg\n",
      "overall train time: 0.219\n",
      "overall test acc: avg 0.864, std 0.004\n",
      "overall test f1: avg 0.888, std 0.004\n",
      "\n",
      "\n",
      "dataset:  Hyperplane\n",
      "concept drifts:  [ 2000  4000  6000  8000 10000 12000 14000 16000]\n",
      "----------------------------------------------------------------------------\n",
      "method:  Full data\n",
      "overall train time: 2.457\n",
      "overall test acc: avg 0.843, std 0.004\n",
      "overall test f1: avg 0.844, std 0.004\n",
      "----------------------------------------------------------------------------\n",
      "method:  Current seg\n",
      "overall train time: 0.516\n",
      "overall test acc: avg 0.893, std 0.004\n",
      "overall test f1: avg 0.894, std 0.004\n",
      "\n",
      "\n",
      "dataset:  RandomRBF\n",
      "concept drifts:  [ 2000  4000  6000  8000 10000 12000 14000 16000]\n",
      "----------------------------------------------------------------------------\n",
      "method:  Full data\n",
      "overall train time: 10.247\n",
      "overall test acc: avg 0.821, std 0.006\n",
      "overall test f1: avg 0.821, std 0.006\n",
      "----------------------------------------------------------------------------\n",
      "method:  Current seg\n",
      "overall train time: 0.586\n",
      "overall test acc: avg 0.679, std 0.010\n",
      "overall test f1: avg 0.673, std 0.011\n",
      "\n",
      "\n",
      "dataset:  Sine\n",
      "concept drifts:  [ 2000  4000  6000  8000 10000 12000 14000 16000]\n",
      "----------------------------------------------------------------------------\n",
      "method:  Full data\n",
      "overall train time: 2.257\n",
      "overall test acc: avg 0.449, std 0.032\n",
      "overall test f1: avg 0.436, std 0.050\n",
      "----------------------------------------------------------------------------\n",
      "method:  Current seg\n",
      "overall train time: 1.029\n",
      "overall test acc: avg 0.899, std 0.004\n",
      "overall test f1: avg 0.898, std 0.004\n",
      "\n",
      "\n",
      "dataset:  Electricity\n",
      "concept drifts:  [ 4320  8640 12960 17280 21600 25920 30240 34560 38880 43200]\n",
      "----------------------------------------------------------------------------\n",
      "method:  Full data\n",
      "overall train time: 7.625\n",
      "overall test acc: avg 0.694, std 0.010\n",
      "overall test f1: avg 0.758, std 0.007\n",
      "----------------------------------------------------------------------------\n",
      "method:  Current seg\n",
      "overall train time: 0.747\n",
      "overall test acc: avg 0.709, std 0.009\n",
      "overall test f1: avg 0.756, std 0.008\n",
      "\n",
      "\n",
      "dataset:  Weather\n",
      "concept drifts:  [ 1800  3600  5400  7200  9000 10800 12600 14400 16200 18000]\n",
      "----------------------------------------------------------------------------\n",
      "method:  Full data\n",
      "overall train time: 4.798\n",
      "overall test acc: avg 0.800, std 0.005\n",
      "overall test f1: avg 0.641, std 0.019\n",
      "----------------------------------------------------------------------------\n",
      "method:  Current seg\n",
      "overall train time: 0.307\n",
      "overall test acc: avg 0.756, std 0.007\n",
      "overall test f1: avg 0.509, std 0.039\n",
      "\n",
      "\n",
      "dataset:  Spam\n",
      "concept drifts:  [1036 2072 3108 4144 5180 6216 7252 8288 9324]\n",
      "----------------------------------------------------------------------------\n",
      "method:  Full data\n",
      "overall train time: 1.312\n",
      "overall test acc: avg 0.970, std 0.003\n",
      "overall test f1: avg 0.973, std 0.002\n",
      "----------------------------------------------------------------------------\n",
      "method:  Current seg\n",
      "overall train time: 0.207\n",
      "overall test acc: avg 0.955, std 0.003\n",
      "overall test f1: avg 0.963, std 0.002\n",
      "\n",
      "\n",
      "dataset:  Usenet1\n",
      "concept drifts:  [ 300  600  900 1200 1500]\n",
      "----------------------------------------------------------------------------\n",
      "method:  Full data\n",
      "overall train time: 0.222\n",
      "overall test acc: avg 0.576, std 0.040\n",
      "overall test f1: avg 0.512, std 0.070\n",
      "----------------------------------------------------------------------------\n",
      "method:  Current seg\n",
      "overall train time: 0.129\n",
      "overall test acc: avg 0.752, std 0.026\n",
      "overall test f1: avg 0.716, std 0.031\n",
      "\n",
      "\n",
      "dataset:  Usenet2\n",
      "concept drifts:  [ 300  600  900 1200 1500]\n",
      "----------------------------------------------------------------------------\n",
      "method:  Full data\n",
      "overall train time: 0.192\n",
      "overall test acc: avg 0.701, std 0.016\n",
      "overall test f1: avg 0.413, std 0.065\n",
      "----------------------------------------------------------------------------\n",
      "method:  Current seg\n",
      "overall train time: 0.192\n",
      "overall test acc: avg 0.745, std 0.018\n",
      "overall test f1: avg 0.613, std 0.026\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_li = ['SEA', 'Hyperplane', 'RandomRBF', 'Sine', 'Electricity', 'Weather', 'Spam', 'Usenet1', 'Usenet2']\n",
    "\n",
    "for dataset in dataset_li:\n",
    "    # load data, label, and concept drift points\n",
    "    x_all = np.load(f'./dataset/{dataset}/data.npy')\n",
    "    y_all = np.load(f'./dataset/{dataset}/label.npy')\n",
    "    concept_drifts = np.load(f'./dataset/{dataset}/concept_drifts.npy')\n",
    "    \n",
    "    # number of classes in dataset\n",
    "    if dataset == 'RandomRBF':\n",
    "        n_class = 5\n",
    "    else:\n",
    "        n_class = 2\n",
    "    \n",
    "    # number of available data in current segment\n",
    "    if dataset in ['Spam', 'Usenet1', 'Usenet2']:\n",
    "        n_train = int((len(x_all)/len(concept_drifts))*0.2)\n",
    "    else:\n",
    "        n_train = int((len(x_all)/len(concept_drifts))*0.1)\n",
    "    \n",
    "    print('dataset: ', dataset)\n",
    "    print('concept drifts: ', concept_drifts)\n",
    "    \n",
    "    method_li = ['Full data', 'Current seg']\n",
    "\n",
    "    for method in method_li:\n",
    "        print(\"----------------------------------------------------------------------------\")\n",
    "        print('method: ', method)\n",
    "\n",
    "        all_time = []\n",
    "        all_acc = []\n",
    "        all_acc_std = []\n",
    "        all_f1 = []\n",
    "        all_f1_std = []\n",
    "\n",
    "        # consecutive evaluation with data segments\n",
    "        for n in range(len(concept_drifts)):\n",
    "            train_time_li = []\n",
    "            test_acc_li = []\n",
    "            test_f1_li = []\n",
    "\n",
    "            n_dataset = n+1\n",
    "            n_feature = x_all.shape[1]\n",
    "\n",
    "            # data preprocessing (scaling)\n",
    "            scaler = StandardScaler()\n",
    "            x_all_scale = scaler.fit_transform(x_all)\n",
    "\n",
    "            # split train, valid, and test set\n",
    "            if n == 0:\n",
    "                x_curr = x_all_scale[:concept_drifts[n]]\n",
    "                y_curr = y_all[:concept_drifts[n]]\n",
    "            else:\n",
    "                x_curr = x_all_scale[concept_drifts[n-1]:concept_drifts[n]]\n",
    "                y_curr = y_all[concept_drifts[n-1]:concept_drifts[n]]\n",
    "\n",
    "            indices = list(range(len(x_curr)))\n",
    "            split1 = int(n_train*0.5)\n",
    "            split2 = n_train\n",
    "            train_indices = indices[:split1]\n",
    "            valid_indices = indices[split1:split2]\n",
    "            test_indices = indices[split2:]\n",
    "\n",
    "            if n == 0:\n",
    "                x_train = x_all_scale[:split1]\n",
    "                y_train = y_all[:split1]\n",
    "            else:\n",
    "                if method == 'Full data':\n",
    "                    x_train = x_all_scale[:concept_drifts[n-1]+split1]\n",
    "                    y_train = y_all[:concept_drifts[n-1]+split1]\n",
    "                elif method == 'Current seg':\n",
    "                    x_train = x_curr[train_indices]\n",
    "                    y_train = y_curr[train_indices]\n",
    "\n",
    "            x_valid = x_curr[valid_indices]\n",
    "            y_valid = y_curr[valid_indices]\n",
    "\n",
    "            x_test = x_curr[test_indices]\n",
    "            y_test = y_curr[test_indices]\n",
    "\n",
    "            x_1 = torch.Tensor(x_train).to(device)\n",
    "            y_1 = torch.Tensor(y_train).to(device, dtype=torch.int64)\n",
    "\n",
    "            x_2 = torch.Tensor(x_valid).to(device)\n",
    "            y_2 = torch.Tensor(y_valid).to(device, dtype=torch.int64)\n",
    "\n",
    "            x_3 = torch.Tensor(x_test).to(device)\n",
    "            y_3 = torch.Tensor(y_test).to(device, dtype=torch.int64)\n",
    "\n",
    "            train_ds = TensorDataset(x_1, y_1)\n",
    "            valid_ds = TensorDataset(x_2, y_2)\n",
    "            test_ds = TensorDataset(x_3, y_3)\n",
    "\n",
    "            train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "            valid_loader = DataLoader(valid_ds, batch_size=128, shuffle=True)\n",
    "            test_loader = DataLoader(test_ds, batch_size=128, shuffle=True)\n",
    "\n",
    "            # repeat experiments with 5 different seeds\n",
    "            for s in range(5):\n",
    "                # initialize model, optimizer, and criterion\n",
    "                model = NormalNN(input_features=n_feature, n_class=n_class, seed=s)\n",
    "                model = model.to(device)\n",
    "                optimizer_config = {\"lr\": 0.001}\n",
    "                clf = NNClassifier(model, nn.CrossEntropyLoss(reduction='mean'), optim.Adam, optimizer_config)\n",
    "\n",
    "                # model training\n",
    "                start = time.time()\n",
    "                clf.fit({\"train\": train_loader, \"val\": valid_loader}, epochs=2000, \n",
    "                        earlystop_path=f'./ckpt/{dataset}_{method}.pt')\n",
    "                train_time = time.time() - start\n",
    "                train_time_li.append(train_time)\n",
    "\n",
    "                # model evaluation\n",
    "                test_output, test_loss = clf.evaluate(test_loader)\n",
    "                test_acc = accuracy_score(test_output['true_y'], test_output['output'])\n",
    "                if n_class > 2:\n",
    "                    test_f1 = f1_score(test_output['true_y'], test_output['output'], average='weighted')\n",
    "                elif n_class == 2:\n",
    "                    test_f1 = f1_score(test_output['true_y'], test_output['output'])\n",
    "                test_acc_li.append(test_acc)\n",
    "                test_f1_li.append(test_f1)\n",
    "\n",
    "            all_time.append(np.mean(train_time_li))\n",
    "            all_acc.append(np.mean(test_acc_li))\n",
    "            all_acc_std.append(np.std(test_acc_li))\n",
    "            all_f1.append(np.mean(test_f1_li))\n",
    "            all_f1_std.append(np.std(test_f1_li))\n",
    "\n",
    "        # print runtime, accuracy, and F1 score\n",
    "        print('overall train time: %.3f' %(np.mean(all_time)))\n",
    "        print('overall test acc: avg %.3f, std %.3f' %(np.mean(all_acc), np.mean(all_acc_std)))\n",
    "        print('overall test f1: avg %.3f, std %.3f' %(np.mean(all_f1), np.mean(all_f1_std)))\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model-centric: HAT, ARF, and Learn++.NSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:  SEA\n",
      "concept drifts:  [ 2000  4000  6000  8000 10000 12000 14000 16000]\n",
      "----------------------------------------------------------------------------\n",
      "method:  HAT\n",
      "overall train time: 1.376\n",
      "overall test acc: avg 0.825, std 0.008\n",
      "overall test f1: avg 0.862, std 0.006\n",
      "----------------------------------------------------------------------------\n",
      "method:  ARF\n",
      "overall train time: 23.657\n",
      "overall test acc: avg 0.825, std 0.008\n",
      "overall test f1: avg 0.863, std 0.008\n",
      "----------------------------------------------------------------------------\n",
      "method:  LearnPPNSE\n",
      "overall train time: 6.637\n",
      "overall test acc: avg 0.803, std 0.006\n",
      "overall test f1: avg 0.835, std 0.005\n",
      "\n",
      "\n",
      "dataset:  Hyperplane\n",
      "concept drifts:  [ 2000  4000  6000  8000 10000 12000 14000 16000]\n",
      "----------------------------------------------------------------------------\n",
      "method:  HAT\n",
      "overall train time: 2.257\n",
      "overall test acc: avg 0.860, std 0.008\n",
      "overall test f1: avg 0.862, std 0.008\n",
      "----------------------------------------------------------------------------\n",
      "method:  ARF\n",
      "overall train time: 27.157\n",
      "overall test acc: avg 0.797, std 0.016\n",
      "overall test f1: avg 0.793, std 0.022\n",
      "----------------------------------------------------------------------------\n",
      "method:  LearnPPNSE\n",
      "overall train time: 7.027\n",
      "overall test acc: avg 0.752, std 0.006\n",
      "overall test f1: avg 0.757, std 0.006\n",
      "\n",
      "\n",
      "dataset:  RandomRBF\n",
      "concept drifts:  [ 2000  4000  6000  8000 10000 12000 14000 16000]\n",
      "----------------------------------------------------------------------------\n",
      "method:  HAT\n",
      "overall train time: 2.362\n",
      "overall test acc: avg 0.514, std 0.010\n",
      "overall test f1: avg 0.519, std 0.011\n",
      "----------------------------------------------------------------------------\n",
      "method:  ARF\n",
      "overall train time: 46.345\n",
      "overall test acc: avg 0.645, std 0.029\n",
      "overall test f1: avg 0.642, std 0.031\n",
      "----------------------------------------------------------------------------\n",
      "method:  LearnPPNSE\n",
      "overall train time: 5.657\n",
      "overall test acc: avg 0.609, std 0.009\n",
      "overall test f1: avg 0.609, std 0.009\n",
      "\n",
      "\n",
      "dataset:  Sine\n",
      "concept drifts:  [ 2000  4000  6000  8000 10000 12000 14000 16000]\n",
      "----------------------------------------------------------------------------\n",
      "method:  HAT\n",
      "overall train time: 1.686\n",
      "overall test acc: avg 0.293, std 0.023\n",
      "overall test f1: avg 0.305, std 0.027\n",
      "----------------------------------------------------------------------------\n",
      "method:  ARF\n",
      "overall train time: 21.305\n",
      "overall test acc: avg 0.821, std 0.050\n",
      "overall test f1: avg 0.823, std 0.049\n",
      "----------------------------------------------------------------------------\n",
      "method:  LearnPPNSE\n",
      "overall train time: 5.730\n",
      "overall test acc: avg 0.925, std 0.004\n",
      "overall test f1: avg 0.925, std 0.004\n",
      "\n",
      "\n",
      "dataset:  Electricity\n",
      "concept drifts:  [ 4320  8640 12960 17280 21600 25920 30240 34560 38880 43200]\n",
      "----------------------------------------------------------------------------\n",
      "method:  HAT\n",
      "overall train time: 6.481\n",
      "overall test acc: avg 0.691, std 0.021\n",
      "overall test f1: avg 0.743, std 0.028\n",
      "----------------------------------------------------------------------------\n",
      "method:  ARF\n",
      "overall train time: 57.548\n",
      "overall test acc: avg 0.713, std 0.011\n",
      "overall test f1: avg 0.762, std 0.014\n",
      "----------------------------------------------------------------------------\n",
      "method:  LearnPPNSE\n",
      "overall train time: 17.507\n",
      "overall test acc: avg 0.699, std 0.003\n",
      "overall test f1: avg 0.735, std 0.003\n",
      "\n",
      "\n",
      "dataset:  Weather\n",
      "concept drifts:  [ 1800  3600  5400  7200  9000 10800 12600 14400 16200 18000]\n",
      "----------------------------------------------------------------------------\n",
      "method:  HAT\n",
      "overall train time: 2.205\n",
      "overall test acc: avg 0.729, std 0.011\n",
      "overall test f1: avg 0.452, std 0.081\n",
      "----------------------------------------------------------------------------\n",
      "method:  ARF\n",
      "overall train time: 32.234\n",
      "overall test acc: avg 0.775, std 0.008\n",
      "overall test f1: avg 0.542, std 0.032\n",
      "----------------------------------------------------------------------------\n",
      "method:  LearnPPNSE\n",
      "overall train time: 7.772\n",
      "overall test acc: avg 0.704, std 0.006\n",
      "overall test f1: avg 0.524, std 0.011\n",
      "\n",
      "\n",
      "dataset:  Spam\n",
      "concept drifts:  [1036 2072 3108 4144 5180 6216 7252 8288 9324]\n",
      "----------------------------------------------------------------------------\n",
      "method:  HAT\n",
      "overall train time: 25.947\n",
      "overall test acc: avg 0.888, std 0.009\n",
      "overall test f1: avg 0.847, std 0.006\n",
      "----------------------------------------------------------------------------\n",
      "method:  ARF\n",
      "overall train time: 52.443\n",
      "overall test acc: avg 0.921, std 0.012\n",
      "overall test f1: avg 0.931, std 0.009\n",
      "----------------------------------------------------------------------------\n",
      "method:  LearnPPNSE\n",
      "overall train time: 4.296\n",
      "overall test acc: avg 0.928, std 0.008\n",
      "overall test f1: avg 0.942, std 0.006\n",
      "\n",
      "\n",
      "dataset:  Usenet1\n",
      "concept drifts:  [ 300  600  900 1200 1500]\n",
      "----------------------------------------------------------------------------\n",
      "method:  HAT\n",
      "overall train time: 0.871\n",
      "overall test acc: avg 0.622, std 0.012\n",
      "overall test f1: avg 0.558, std 0.048\n",
      "----------------------------------------------------------------------------\n",
      "method:  ARF\n",
      "overall train time: 4.203\n",
      "overall test acc: avg 0.629, std 0.055\n",
      "overall test f1: avg 0.616, std 0.062\n",
      "----------------------------------------------------------------------------\n",
      "method:  LearnPPNSE\n",
      "overall train time: 0.363\n",
      "overall test acc: avg 0.443, std 0.011\n",
      "overall test f1: avg 0.417, std 0.014\n",
      "\n",
      "\n",
      "dataset:  Usenet2\n",
      "concept drifts:  [ 300  600  900 1200 1500]\n",
      "----------------------------------------------------------------------------\n",
      "method:  HAT\n",
      "overall train time: 0.884\n",
      "overall test acc: avg 0.730, std 0.009\n",
      "overall test f1: avg 0.472, std 0.022\n",
      "----------------------------------------------------------------------------\n",
      "method:  ARF\n",
      "overall train time: 4.248\n",
      "overall test acc: avg 0.682, std 0.036\n",
      "overall test f1: avg 0.311, std 0.138\n",
      "----------------------------------------------------------------------------\n",
      "method:  LearnPPNSE\n",
      "overall train time: 0.344\n",
      "overall test acc: avg 0.636, std 0.011\n",
      "overall test f1: avg 0.256, std 0.023\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_li = ['SEA', 'Hyperplane', 'RandomRBF', 'Sine', 'Electricity', 'Weather', 'Spam', 'Usenet1', 'Usenet2']\n",
    "\n",
    "for dataset in dataset_li:\n",
    "    # load data, label, and concept drift points\n",
    "    x_all = np.load(f'./dataset/{dataset}/data.npy')\n",
    "    y_all = np.load(f'./dataset/{dataset}/label.npy')\n",
    "    concept_drifts = np.load(f'./dataset/{dataset}/concept_drifts.npy')\n",
    "    \n",
    "    # number of classes in dataset\n",
    "    if dataset == 'RandomRBF':\n",
    "        n_class = 5\n",
    "    else:\n",
    "        n_class = 2\n",
    "    \n",
    "    # number of available data in current segment\n",
    "    if dataset in ['Spam', 'Usenet1', 'Usenet2']:\n",
    "        n_train = int((len(x_all)/len(concept_drifts))*0.2)\n",
    "    else:\n",
    "        n_train = int((len(x_all)/len(concept_drifts))*0.1)\n",
    "\n",
    "    print('dataset: ', dataset)\n",
    "    print('concept drifts: ', concept_drifts)\n",
    "    \n",
    "    method_li = ['HAT', 'ARF', 'LearnPPNSE']\n",
    "\n",
    "    for method in method_li:\n",
    "        print(\"----------------------------------------------------------------------------\")\n",
    "        print('method: ', method)\n",
    "\n",
    "        all_time = []\n",
    "        all_acc = []\n",
    "        all_acc_std = []\n",
    "        all_f1 = []\n",
    "        all_f1_std = []\n",
    "\n",
    "        # consecutive evaluation with data segments\n",
    "        for n in range(len(concept_drifts)):\n",
    "            n_dataset = n+1\n",
    "            n_feature = x_all.shape[1]\n",
    "\n",
    "            # data preprocessing (scaling)\n",
    "            scaler = StandardScaler()\n",
    "            x_all_scale = scaler.fit_transform(x_all)\n",
    "            \n",
    "            dataset_final = range(n+1)\n",
    "        \n",
    "            # split train and test set\n",
    "            if n == 0:\n",
    "                x_curr = x_all_scale[:concept_drifts[n]]\n",
    "                y_curr = y_all[:concept_drifts[n]]\n",
    "            else:\n",
    "                x_curr = x_all_scale[concept_drifts[n-1]:concept_drifts[n]]\n",
    "                y_curr = y_all[concept_drifts[n-1]:concept_drifts[n]]\n",
    "\n",
    "            indices = list(range(len(x_curr)))\n",
    "            split1 = int(n_train*0.5)\n",
    "            split2 = n_train\n",
    "            train_indices = indices[:split1]\n",
    "            valid_indices = indices[split1:split2]\n",
    "            test_indices = indices[split2:]\n",
    "\n",
    "            if n == 0:\n",
    "                x_train = np.concatenate((x_curr[train_indices], x_curr[valid_indices]))\n",
    "                y_train = np.concatenate((y_curr[train_indices], y_curr[valid_indices])).reshape(-1,1)\n",
    "\n",
    "            else:\n",
    "                x_train = np.concatenate((x_all_scale[:concept_drifts[n-1]], x_curr[train_indices], \n",
    "                                          x_curr[valid_indices]))\n",
    "                y_train = np.concatenate((y_all[:concept_drifts[n-1]], y_curr[train_indices], \n",
    "                                          y_curr[valid_indices])).reshape(-1,1)\n",
    "\n",
    "            x_test = x_curr[test_indices]\n",
    "            y_test = y_curr[test_indices].reshape(-1,1)\n",
    "\n",
    "            train_time_li = []\n",
    "            test_acc_li = []\n",
    "            test_f1_li = []\n",
    "\n",
    "            # repeat experiments with 5 different seeds\n",
    "            for s in range(5):\n",
    "                # initialize model\n",
    "                if method == 'HAT':\n",
    "                    model = HoeffdingAdaptiveTreeClassifier(random_state=s, grace_period=n_train)\n",
    "                elif method == 'ARF':\n",
    "                    model = AdaptiveRandomForestClassifier(random_state=s, grace_period=n_train)\n",
    "                elif method == 'LearnPPNSE':\n",
    "                    model = LearnPPNSEClassifier(window_size=n_train)\n",
    "\n",
    "                # model training\n",
    "                start = time.time()\n",
    "                for i in range(len(x_train)):\n",
    "                    X = np.array([x_train[i]])\n",
    "                    y = y_train[i]\n",
    "                    if method == 'HAT' or method == 'ARF':\n",
    "                        model.partial_fit(X, y)\n",
    "                    elif method == 'LearnPPNSE':\n",
    "                        model.partial_fit(X, y, classes=np.array(range(n_class)))\n",
    "                train_time = time.time() - start\n",
    "                train_time_li.append(train_time)\n",
    "\n",
    "                # model evaluation\n",
    "                n_samples = 0\n",
    "                correct_cnt = 0\n",
    "                \n",
    "                y_pred = []\n",
    "                y_truth = []\n",
    "\n",
    "                for i in range(len(x_test)):\n",
    "                    X = np.array([x_test[i]])\n",
    "                    y = y_test[i]\n",
    "                    y_pred.append(model.predict(X)[0])\n",
    "                    y_truth.append(y[0])\n",
    "\n",
    "                test_acc = accuracy_score(y_truth, y_pred)\n",
    "                if n_class > 2:\n",
    "                    test_f1 = f1_score(y_truth, y_pred, average='weighted')\n",
    "                elif n_class == 2:\n",
    "                    test_f1 = f1_score(y_truth, y_pred)\n",
    "                test_acc_li.append(test_acc)\n",
    "                test_f1_li.append(test_f1)\n",
    "\n",
    "            all_time.append(np.mean(train_time_li))\n",
    "            all_acc.append(np.mean(test_acc_li))\n",
    "            all_acc_std.append(np.std(test_acc_li))\n",
    "            all_f1.append(np.mean(test_f1_li))\n",
    "            all_f1_std.append(np.std(test_f1_li))\n",
    "\n",
    "        # print runtime, accuracy, and F1 score\n",
    "        print('overall train time: %.3f' %(np.mean(all_time)))\n",
    "        print('overall test acc: avg %.3f, std %.3f' %(np.mean(all_acc), np.mean(all_acc_std)))\n",
    "        print('overall test f1: avg %.3f, std %.3f' %(np.mean(all_f1), np.mean(all_f1_std)))\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model-centric: SEGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of the SEGA paper.\n",
    "A Segment-Based Drift Adaptation Method for Data Streams.\n",
    "citation:\n",
    "@article{DBLP:journals/tnn/SongLLLZ22,\n",
    "  author       = {Yiliao Song and\n",
    "                  Jie Lu and\n",
    "                  Anjin Liu and\n",
    "                  Haiyan Lu and\n",
    "                  Guangquan Zhang},\n",
    "  title        = {A Segment-Based Drift Adaptation Method for Data Streams},\n",
    "  journal      = {{IEEE} Trans. Neural Networks Learn. Syst.},\n",
    "  volume       = {33},\n",
    "  number       = {9},\n",
    "  pages        = {4876--4889},\n",
    "  year         = {2022},\n",
    "  url          = {https://doi.org/10.1109/TNNLS.2021.3062062},\n",
    "  doi          = {10.1109/TNNLS.2021.3062062},\n",
    "  timestamp    = {Thu, 22 Sep 2022 19:58:23 +0200},\n",
    "  biburl       = {https://dblp.org/rec/journals/tnn/SongLLLZ22.bib},\n",
    "  bibsource    = {dblp computer science bibliography, https://dblp.org}\n",
    "}\n",
    "\"\"\"\n",
    "class IncLDD_C:\n",
    "    def __str__(self):\n",
    "        return \"Class: Inc_LDD_stream_classification\"\n",
    "    \n",
    "    def __init__(self, dataset, n_train, base_learner, knowledge_base_size=1000, win_size=100, para_KNN = 100):        \n",
    "        self.data = dataset\n",
    "        self.n_train = n_train\n",
    "        self.learner = base_learner \n",
    "        self.win_size = win_size         \n",
    "        self.kb_size = knowledge_base_size\n",
    "        self.KNN_size = para_KNN\n",
    "        self.predictions = []       \n",
    "    \n",
    "    def run_online(self):\n",
    "        data = self.data\n",
    "        n_size_data = self.data.shape[0]        \n",
    "        knowledgebase = self.data[:self.kb_size, :]\n",
    "        num_kb_win = math.ceil(self.kb_size/self.win_size)\n",
    "        error_debug = []\n",
    "        \n",
    "        predictions = []\n",
    "        buffer = np.ndarray(shape=(0,data.shape[1]), dtype=float)                \n",
    "           \n",
    "        kb_data_list = np.split(knowledgebase,num_kb_win,axis=0)    \n",
    "        knowledgebase_norm,min_max_data = scale_linear_bycolumn(knowledgebase)\n",
    "        kb_ssd_array, dist_indx = compSSD(knowledgebase_norm,num_kb_win,self.KNN_size)\n",
    "        kb_ssd_array_0 = kb_ssd_array\n",
    "        \n",
    "        learner_list = []\n",
    "        for learn_data  in kb_data_list:\n",
    "            _learner = self.learner\n",
    "            learner_list.append(clone(_learner).fit(learn_data[:,:-1],learn_data[:,-1]))\n",
    "                    \n",
    "        K_uq = np.zeros(shape=(self.kb_size))\n",
    "        K_vp = np.zeros(shape=(num_kb_win))\n",
    "        Oq = np.ndarray(shape = 0)\n",
    "        \n",
    "        for i in range(self.kb_size, n_size_data):\n",
    "            \n",
    "            min_ssd_idx = np.argsort(kb_ssd_array)[0:5]\n",
    "   \n",
    "            learner_predictions = []                \n",
    "            for _learner_indx in min_ssd_idx:\n",
    "                learner_predictions.append(learner_list[_learner_indx].predict([data[i,:-1]])[0])\n",
    "                if i < self.kb_size + self.n_train:\n",
    "                    learn_data = np.vstack([kb_data_list[_learner_indx], data[i,:]])\n",
    "                    learner_list[_learner_indx] = learner_list[_learner_indx].fit(learn_data[:,:-1],learn_data[:,-1])\n",
    "            error_debug.append(learner_predictions)\n",
    "            predictions.append(max(set(learner_predictions), key=learner_predictions.count))\n",
    "    \n",
    "            if i < self.kb_size + self.n_train and buffer.shape[0] == self.win_size:\n",
    "                K_vp, K_uq, Oq = incrOnS(knowledgebase_norm, data[i,:], min_max_data, dist_indx, num_kb_win, K_uq, K_vp, Oq, self.KNN_size)         \n",
    "                deltaSSD = -K_vp - (1+1/num_kb_win)*np.sum(np.split(K_uq,num_kb_win),axis=1)\n",
    "                kb_ssd_array = deltaSSD\n",
    "                kb_ssd_array_0 = deltaSSD + kb_ssd_array_0\n",
    "                buffer = np.vstack([buffer, data[i,:]])\n",
    "            \n",
    "            \n",
    "            if i < self.kb_size + self.n_train and buffer.shape[0] == self.win_size:\n",
    "                knowledgebase =  np.concatenate((knowledgebase,buffer),axis = 0)[-self.kb_size:,:]\n",
    "               \n",
    "                learner_buffer = clone(self.learner).fit(buffer[:,:-1],buffer[:,-1])\n",
    "                learner_list.append(learner_buffer)\n",
    "                del learner_list[0]\n",
    "               \n",
    "                buffer = np.ndarray(shape=(0,data.shape[1]), dtype=float)\n",
    "               \n",
    "                kb_data_list = np.split(knowledgebase,num_kb_win,axis=0)    \n",
    "                knowledgebase_norm,min_max_data = scale_linear_bycolumn(knowledgebase)\n",
    "                kb_ssd_array, dist_indx = compSSD(knowledgebase_norm,num_kb_win,self.KNN_size)\n",
    "                kb_ssd_array_0 = kb_ssd_array\n",
    "\n",
    "                K_uq = np.zeros(shape=(self.kb_size))\n",
    "                K_vp = np.zeros(shape=(num_kb_win))\n",
    "                Oq = np.ndarray(shape = 0)\n",
    "\n",
    "        predictions = predictions[n_train:]\n",
    "\n",
    "        acc = metrics.accuracy_score(self.data[self.kb_size+n_train:n_size_data, -1], predictions)\n",
    "\n",
    "        return acc,self.data[self.kb_size+n_train:n_size_data, -1:],np.asarray(predictions),np.asarray(error_debug)\n",
    "       \n",
    "def incrOnS(knowledgebase_norm,newinstance,min_max_data,dist_indx,num_kb_win,K_uq,K_vp,Oq,para_KNN=100):\n",
    "    \n",
    "    _instance = np.ndarray(shape=(1,len(newinstance))); _instance[0,:] = newinstance \n",
    "    _instance_norm, __ = scale_linear_bycolumn(_instance,min_max_data=min_max_data)\n",
    "    \n",
    "    Np = int(knowledgebase_norm.shape[0])\n",
    "    Nq = int(knowledgebase_norm.shape[0]/num_kb_win)\n",
    "    \n",
    "    distances_list = dist_indx[0]\n",
    "    indx_seg = dist_indx[2]\n",
    "    nbrs = NearestNeighbors(n_neighbors=knowledgebase_norm.shape[0],algorithm = 'ball_tree').fit(knowledgebase_norm)\n",
    "    _distances_0,_indices_0 = nbrs.kneighbors(_instance_norm)  \n",
    "    _distances = np.zeros(shape=(1,_distances_0.shape[1]))\n",
    "    _distances[0][_indices_0[0]] = _distances_0[0]\n",
    "    _indices = _indices_0[0][:para_KNN]\n",
    "    \n",
    "    delta_K_vp = np.ndarray(shape=(0))\n",
    "    for _seg in indx_seg:\n",
    "        _K_vp =len(np.intersect1d(_indices,_seg))\n",
    "        delta_K_vp = np.append(delta_K_vp,[_K_vp/Np/Nq],axis=0)\n",
    "    K_vp = delta_K_vp \n",
    "               \n",
    "    return K_vp, K_uq, Oq    \n",
    "        \n",
    "def compSSD(knowledgebase_norm, num_kb_win, para_KNN = 100):    \n",
    "    Np = int(knowledgebase_norm.shape[0] - knowledgebase_norm.shape[0]/num_kb_win)\n",
    "    Nq = int(knowledgebase_norm.shape[0]/num_kb_win)\n",
    "    \n",
    "    fitbase = knowledgebase_norm\n",
    "    nbrs = NearestNeighbors(n_neighbors=para_KNN,algorithm = 'ball_tree').fit(fitbase) #Ntrain-Nseg\n",
    "\n",
    "    distances,indices = nbrs.kneighbors(fitbase)\n",
    "    indicesP = indices[:Np,:]; indicesQ = indices[Np:,:];\n",
    "    K_up = (indicesP<=Np).sum(1)  \n",
    "    K_uq = (indicesP>Np).sum(1)   \n",
    "    \n",
    "    K_vq = (indicesQ>=Np).sum(1)\n",
    "    K_vp = (indicesQ<Np).sum(1)\n",
    "    \n",
    "    sd_P = (K_up/Np-K_uq/Nq)/Np\n",
    "    sd_Q = (K_vq/Nq-K_vp/Np)/Nq\n",
    "    sd = sd_P.sum()+sd_Q.sum()\n",
    "    \n",
    "    ssd_p = np.split(sd_P,num_kb_win-1,axis=0)\n",
    "    ssd = np.zeros(shape = num_kb_win)\n",
    "    ssd[0:-1] = np.sum(ssd_p,1)+sd_Q.sum()/(num_kb_win-1)\n",
    "    \n",
    "    indx_base = np.arange(0,knowledgebase_norm.shape[0],1)\n",
    "    indx_seg = np.split(indx_base,num_kb_win,axis=0) \n",
    "    distances_list = np.ndarray.tolist(distances)\n",
    "    dist_indx = [distances_list,indices,indx_seg]\n",
    "    return ssd, dist_indx  \n",
    "            \n",
    "def scale_linear_bycolumn(data, high=1, low=0,  min_max_data=None):\n",
    "    if min_max_data is None:\n",
    "        mins_data = np.min(data, axis=0)\n",
    "        maxs_data = np.max(data, axis=0)\n",
    "        avg_data = np.mean(data, axis=0)\n",
    "        std_data = np.std(data, axis = 0)\n",
    "    else:\n",
    "        mins_data = min_max_data[0]\n",
    "        maxs_data = min_max_data[1]\n",
    "        avg_data = min_max_data[2]\n",
    "        std_data = min_max_data[3]\n",
    "    rng = maxs_data - mins_data\n",
    "    rng[rng==0]=1\n",
    "    \n",
    "    data_norm = high - (((high - low) * (maxs_data - data)) / rng)\n",
    "\n",
    "    data_norm[:,-1] = data_norm[:,-1]*(data.shape[1]-1)\n",
    "\n",
    "    return data_norm, [mins_data, maxs_data, avg_data, std_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:  SEA\n",
      "concept drifts:  [ 2000  4000  6000  8000 10000 12000 14000 16000]\n",
      "----------------------------------------------------------------------------\n",
      "method: SEGA\n",
      "overall train time: 4.466\n",
      "overall test acc: avg 0.797, std 0.000\n",
      "overall test f1: avg 0.842, std 0.000\n",
      "\n",
      "\n",
      "dataset:  Hyperplane\n",
      "concept drifts:  [ 2000  4000  6000  8000 10000 12000 14000 16000]\n",
      "----------------------------------------------------------------------------\n",
      "method: SEGA\n",
      "overall train time: 4.519\n",
      "overall test acc: avg 0.853, std 0.000\n",
      "overall test f1: avg 0.851, std 0.000\n",
      "\n",
      "\n",
      "dataset:  RandomRBF\n",
      "concept drifts:  [ 2000  4000  6000  8000 10000 12000 14000 16000]\n",
      "----------------------------------------------------------------------------\n",
      "method: SEGA\n",
      "overall train time: 4.509\n",
      "overall test acc: avg 0.825, std 0.000\n",
      "overall test f1: avg 0.825, std 0.000\n",
      "\n",
      "\n",
      "dataset:  Sine\n",
      "concept drifts:  [ 2000  4000  6000  8000 10000 12000 14000 16000]\n",
      "----------------------------------------------------------------------------\n",
      "method: SEGA\n",
      "overall train time: 4.387\n",
      "overall test acc: avg 0.253, std 0.000\n",
      "overall test f1: avg 0.260, std 0.000\n",
      "\n",
      "\n",
      "dataset:  Electricity\n",
      "concept drifts:  [ 4320  8640 12960 17280 21600 25920 30240 34560 38880 43200]\n",
      "----------------------------------------------------------------------------\n",
      "method: SEGA\n",
      "overall train time: 10.313\n",
      "overall test acc: avg 0.637, std 0.000\n",
      "overall test f1: avg 0.697, std 0.000\n",
      "\n",
      "\n",
      "dataset:  Weather\n",
      "concept drifts:  [ 1800  3600  5400  7200  9000 10800 12600 14400 16200 18000]\n",
      "----------------------------------------------------------------------------\n",
      "method: SEGA\n",
      "overall train time: 4.115\n",
      "overall test acc: avg 0.777, std 0.000\n",
      "overall test f1: avg 0.602, std 0.000\n",
      "\n",
      "\n",
      "dataset:  Spam\n",
      "concept drifts:  [1036 2072 3108 4144 5180 6216 7252 8288 9324]\n",
      "----------------------------------------------------------------------------\n",
      "method: SEGA\n",
      "overall train time: 6.594\n",
      "overall test acc: avg 0.858, std 0.000\n",
      "overall test f1: avg 0.851, std 0.000\n",
      "\n",
      "\n",
      "dataset:  Usenet1\n",
      "concept drifts:  [ 300  600  900 1200 1500]\n",
      "----------------------------------------------------------------------------\n",
      "method: SEGA\n",
      "overall train time: 0.834\n",
      "overall test acc: avg 0.403, std 0.000\n",
      "overall test f1: avg 0.318, std 0.000\n",
      "\n",
      "\n",
      "dataset:  Usenet2\n",
      "concept drifts:  [ 300  600  900 1200 1500]\n",
      "----------------------------------------------------------------------------\n",
      "method: SEGA\n",
      "overall train time: 0.830\n",
      "overall test acc: avg 0.630, std 0.000\n",
      "overall test f1: avg 0.207, std 0.000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_li = ['SEA', 'Hyperplane', 'RandomRBF', 'Sine', 'Electricity', 'Weather', 'Spam', 'Usenet1', 'Usenet2']\n",
    "    \n",
    "for dataset in dataset_li:\n",
    "    # load data, label, and concept drift points\n",
    "    x_all = np.load(f'./dataset/{dataset}/data.npy')\n",
    "    y_all = np.load(f'./dataset/{dataset}/label.npy')\n",
    "    concept_drifts = np.load(f'./dataset/{dataset}/concept_drifts.npy')\n",
    "\n",
    "    # number of classes in dataset\n",
    "    if dataset == 'RandomRBF':\n",
    "        n_class = 5\n",
    "    else:\n",
    "        n_class = 2\n",
    "\n",
    "    # number of available data in current segment\n",
    "    if dataset in ['Spam', 'Usenet1', 'Usenet2']:\n",
    "        n_train = int((len(x_all)/len(concept_drifts))*0.2)\n",
    "    else:\n",
    "        n_train = int((len(x_all)/len(concept_drifts))*0.1)\n",
    "\n",
    "    # set window size\n",
    "    win_size = n_train\n",
    "\n",
    "    # set historical buffer size\n",
    "    if dataset in ['Spam', 'Usenet1', 'Usenet2']:\n",
    "        hist_size = 5\n",
    "    else:\n",
    "        hist_size = 10\n",
    "\n",
    "    # set knowledge base size\n",
    "    knowledge_base_size = hist_size*win_size\n",
    "\n",
    "    print('dataset: ', dataset)\n",
    "    print('concept drifts: ', concept_drifts)\n",
    "\n",
    "    sega_time = []\n",
    "    sega_acc = []\n",
    "    sega_acc_std = []\n",
    "    sega_f1 = []\n",
    "    sega_f1_std = []\n",
    "    \n",
    "    print(\"----------------------------------------------------------------------------\")\n",
    "    print('method: SEGA')\n",
    "\n",
    "    # consecutive evaluation with data segments\n",
    "    for n in range(len(concept_drifts)):\n",
    "        sega_time_li = []\n",
    "        sega_acc_li = []\n",
    "        sega_f1_li = []\n",
    "\n",
    "        # repeat experiments with 5 different seeds\n",
    "        for s in range(5):\n",
    "            # for the first segment, there is no knowledge base\n",
    "            if n == 0:\n",
    "                neigh = neighbors.KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "                start = time.time()\n",
    "                neigh.fit(x_all[:n_train], y_all[:n_train])\n",
    "                end = time.time()\n",
    "                y_pred = neigh.predict(x_all[n_train:int(len(x_all)/len(concept_drifts))])\n",
    "                y_truth = y_all[n_train:int(len(x_all)/len(concept_drifts))]\n",
    "\n",
    "                sega_time_li.append(end-start)\n",
    "                sega_acc_li.append(accuracy_score(y_truth, y_pred))\n",
    "                if n_class > 2:\n",
    "                    sega_f1_li.append(f1_score(y_truth, y_pred, average='weighted'))\n",
    "                elif n_class == 2:\n",
    "                    sega_f1_li.append(f1_score(y_truth, y_pred))\n",
    "                    \n",
    "            # after the first segment, use previous knowledge base for prediction\n",
    "            else:\n",
    "                x_temp = x_all[(n-1)*int(len(x_all)/len(concept_drifts)):(n+1)*int(len(x_all)/len(concept_drifts))]\n",
    "                y_temp = y_all[(n-1)*int(len(x_all)/len(concept_drifts)):(n+1)*int(len(x_all)/len(concept_drifts))]\n",
    "\n",
    "                _data = np.hstack((x_temp,y_temp.reshape(len(y_temp),1)))\n",
    "\n",
    "                neigh = neighbors.KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "                _predictor = neigh \n",
    "                _incLDD_C = IncLDD_C(_data, n_train, _predictor, knowledge_base_size, win_size, para_KNN = int(n_train/2))   \n",
    "\n",
    "                start = time.time()\n",
    "                acc_result, realvalue, predvalue, error_debug = _incLDD_C.run_online()\n",
    "                end = time.time()\n",
    "                sega_time_li.append(end-start)\n",
    "\n",
    "                y_pred = predvalue\n",
    "                y_truth = realvalue\n",
    "\n",
    "                sega_acc_li.append(accuracy_score(y_truth, y_pred))\n",
    "                if n_class > 2:\n",
    "                    sega_f1_li.append(f1_score(y_truth, y_pred, average='weighted'))\n",
    "                elif n_class == 2:\n",
    "                    sega_f1_li.append(f1_score(y_truth, y_pred))\n",
    "\n",
    "        sega_time.append(np.mean(sega_time_li))\n",
    "        sega_acc.append(np.mean(sega_acc_li))\n",
    "        sega_acc_std.append(np.std(sega_acc_li))\n",
    "        sega_f1.append(np.mean(sega_f1_li))\n",
    "        sega_f1_std.append(np.std(sega_f1_li))\n",
    "\n",
    "    # print runtime, accuracy, and F1 score\n",
    "    print('overall train time: %.3f' %(np.mean(sega_time)))\n",
    "    print('overall test acc: avg %.3f, std %.3f' %(np.mean(sega_acc), np.mean(sega_acc_std)))\n",
    "    print('overall test f1: avg %.3f, std %.3f' %(np.mean(sega_f1), np.mean(sega_f1_std)))\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data-centric: CVDTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:  SEA\n",
      "concept drifts:  [ 2000  4000  6000  8000 10000 12000 14000 16000]\n",
      "----------------------------------------------------------------------------\n",
      "method:  CVDTE\n",
      "overall train time: 0.023\n",
      "overall test acc: avg 0.806, std 0.018\n",
      "overall test f1: avg 0.812, std 0.047\n",
      "\n",
      "\n",
      "dataset:  Hyperplane\n",
      "concept drifts:  [ 2000  4000  6000  8000 10000 12000 14000 16000]\n",
      "----------------------------------------------------------------------------\n",
      "method:  CVDTE\n",
      "overall train time: 0.038\n",
      "overall test acc: avg 0.744, std 0.007\n",
      "overall test f1: avg 0.752, std 0.015\n",
      "\n",
      "\n",
      "dataset:  RandomRBF\n",
      "concept drifts:  [ 2000  4000  6000  8000 10000 12000 14000 16000]\n",
      "----------------------------------------------------------------------------\n",
      "method:  CVDTE\n",
      "overall train time: 0.053\n",
      "overall test acc: avg 0.614, std 0.015\n",
      "overall test f1: avg 0.621, std 0.023\n",
      "\n",
      "\n",
      "dataset:  Sine\n",
      "concept drifts:  [ 2000  4000  6000  8000 10000 12000 14000 16000]\n",
      "----------------------------------------------------------------------------\n",
      "method:  CVDTE\n",
      "overall train time: 0.017\n",
      "overall test acc: avg 0.857, std 0.004\n",
      "overall test f1: avg 0.835, std 0.026\n",
      "\n",
      "\n",
      "dataset:  Electricity\n",
      "concept drifts:  [ 4320  8640 12960 17280 21600 25920 30240 34560 38880 43200]\n",
      "----------------------------------------------------------------------------\n",
      "method:  CVDTE\n",
      "overall train time: 0.041\n",
      "overall test acc: avg 0.689, std 0.010\n",
      "overall test f1: avg 0.736, std 0.013\n",
      "\n",
      "\n",
      "dataset:  Weather\n",
      "concept drifts:  [ 1800  3600  5400  7200  9000 10800 12600 14400 16200 18000]\n",
      "----------------------------------------------------------------------------\n",
      "method:  CVDTE\n",
      "overall train time: 0.033\n",
      "overall test acc: avg 0.731, std 0.009\n",
      "overall test f1: avg 0.497, std 0.029\n",
      "\n",
      "\n",
      "dataset:  Spam\n",
      "concept drifts:  [1036 2072 3108 4144 5180 6216 7252 8288 9324]\n",
      "----------------------------------------------------------------------------\n",
      "method:  CVDTE\n",
      "overall train time: 0.123\n",
      "overall test acc: avg 0.917, std 0.009\n",
      "overall test f1: avg 0.918, std 0.017\n",
      "\n",
      "\n",
      "dataset:  Usenet1\n",
      "concept drifts:  [ 300  600  900 1200 1500]\n",
      "----------------------------------------------------------------------------\n",
      "method:  CVDTE\n",
      "overall train time: 0.006\n",
      "overall test acc: avg 0.718, std 0.007\n",
      "overall test f1: avg 0.696, std 0.036\n",
      "\n",
      "\n",
      "dataset:  Usenet2\n",
      "concept drifts:  [ 300  600  900 1200 1500]\n",
      "----------------------------------------------------------------------------\n",
      "method:  CVDTE\n",
      "overall train time: 0.006\n",
      "overall test acc: avg 0.689, std 0.012\n",
      "overall test f1: avg 0.523, std 0.021\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_li = ['SEA', 'Hyperplane', 'RandomRBF', 'Sine', 'Electricity', 'Weather', 'Spam', 'Usenet1', 'Usenet2']\n",
    "\n",
    "for dataset in dataset_li:\n",
    "    # load data, label, and concept drift points\n",
    "    x_all = np.load(f'./dataset/{dataset}/data.npy')\n",
    "    y_all = np.load(f'./dataset/{dataset}/label.npy')\n",
    "    concept_drifts = np.load(f'./dataset/{dataset}/concept_drifts.npy')\n",
    "    \n",
    "    # number of classes in dataset\n",
    "    if dataset == 'RandomRBF':\n",
    "        n_class = 5\n",
    "    else:\n",
    "        n_class = 2\n",
    "    \n",
    "    # number of available data in current segment\n",
    "    if dataset in ['Spam', 'Usenet1', 'Usenet2']:\n",
    "        n_train = int((len(x_all)/len(concept_drifts))*0.2)\n",
    "    else:\n",
    "        n_train = int((len(x_all)/len(concept_drifts))*0.1)\n",
    "\n",
    "    print('dataset: ', dataset)\n",
    "    print('concept drifts: ', concept_drifts)\n",
    "    \n",
    "    method_li = ['CVDTE']\n",
    "\n",
    "    for method in method_li:\n",
    "        print(\"----------------------------------------------------------------------------\")\n",
    "        print('method: ', method)\n",
    "\n",
    "        all_time = []\n",
    "        all_acc = []\n",
    "        all_acc_std = []\n",
    "        all_f1 = []\n",
    "        all_f1_std = []\n",
    "\n",
    "        # consecutive evaluation with data segments\n",
    "        for n in range(len(concept_drifts)):\n",
    "            n_dataset = n+1\n",
    "            n_feature = x_all.shape[1]\n",
    "\n",
    "            # data preprocessing (scaling)\n",
    "            scaler = StandardScaler()\n",
    "            x_all_scale = scaler.fit_transform(x_all)\n",
    "\n",
    "            # first data segment case (only FN possible)\n",
    "            if n == 0:\n",
    "                train_time_li = []\n",
    "                test_acc_li = []\n",
    "                \n",
    "                # split train, valid, and test set\n",
    "                x_curr = x_all_scale[:concept_drifts[n]]\n",
    "                y_curr = y_all[:concept_drifts[n]]\n",
    "\n",
    "                indices = list(range(len(x_curr)))\n",
    "                split1 = int(n_train*0.5)\n",
    "                split2 = n_train\n",
    "                train_indices = indices[:split1]\n",
    "                valid_indices = indices[split1:split2]\n",
    "                test_indices = indices[split2:]\n",
    "\n",
    "                x_train = x_curr[train_indices]\n",
    "                y_train = y_curr[train_indices]\n",
    "\n",
    "                x_valid = x_curr[valid_indices]\n",
    "                y_valid = y_curr[valid_indices]\n",
    "\n",
    "                x_test = x_curr[test_indices]\n",
    "                y_test = y_curr[test_indices]\n",
    "\n",
    "                # repeat experiments with 5 different seeds\n",
    "                for s in range(5):\n",
    "                    start = time.time()\n",
    "\n",
    "                    # train model with new data (FN)\n",
    "                    model = DecisionTreeClassifier(random_state=s)\n",
    "                    model.fit(x_train, y_train)\n",
    "\n",
    "                    train_time = time.time() - start\n",
    "                    train_time_li.append(train_time)\n",
    "\n",
    "                    # model evaluation\n",
    "                    y_pred = model.predict(x_test)\n",
    "                    y_truth = y_test\n",
    "                    \n",
    "                    test_acc = accuracy_score(y_truth, y_pred)\n",
    "                    if n_class > 2:\n",
    "                        test_f1 = f1_score(y_truth, y_pred, average='weighted')\n",
    "                    elif n_class == 2:\n",
    "                        test_f1 = f1_score(y_truth, y_pred)\n",
    "                    test_acc_li.append(test_acc)\n",
    "                    test_f1_li.append(test_f1)\n",
    "\n",
    "                    pickle.dump(model, open(f'./ckpt/CVDTE_{0}_{s}', 'wb'))\n",
    "\n",
    "                all_time.append(np.mean(train_time_li))\n",
    "                all_acc.append(np.mean(test_acc_li))\n",
    "                all_acc_std.append(np.std(test_acc_li))\n",
    "                all_f1.append(np.mean(test_f1_li))\n",
    "                all_f1_std.append(np.std(test_f1_li))\n",
    "\n",
    "            # next data segment case (FN, FN+, FO, FO+ possible)\n",
    "            else:\n",
    "                # split train, valid, and test set\n",
    "                x_curr = x_all_scale[concept_drifts[n-1]:concept_drifts[n]]\n",
    "                y_curr = y_all[concept_drifts[n-1]:concept_drifts[n]]\n",
    "\n",
    "                indices = list(range(len(x_curr)))\n",
    "                split1 = int(n_train*0.5)\n",
    "                split2 = n_train\n",
    "                train_indices = indices[:split1]\n",
    "                valid_indices = indices[split1:split2]\n",
    "                test_indices = indices[split2:]\n",
    "\n",
    "                x_train = x_curr[train_indices]\n",
    "                y_train = y_curr[train_indices]\n",
    "\n",
    "                x_valid = x_curr[valid_indices]\n",
    "                y_valid = y_curr[valid_indices]\n",
    "\n",
    "                x_test = x_curr[test_indices]\n",
    "                y_test = y_curr[test_indices]\n",
    "\n",
    "                train_time_li = []\n",
    "                test_acc_li = []\n",
    "                test_f1_li = []\n",
    "\n",
    "                # repeat experiments with 5 different seeds\n",
    "                for s in range(5):\n",
    "                    start = time.time()\n",
    "\n",
    "                    # train model with new data (FN)\n",
    "                    model_fn = DecisionTreeClassifier(random_state=s)\n",
    "                    model_fn.fit(x_train, y_train)\n",
    "\n",
    "                    # find previous useful data\n",
    "                    useful_sample = []\n",
    "\n",
    "                    for j in range(1, n+1):\n",
    "\n",
    "                        if j == 1:\n",
    "                            init = 0\n",
    "                        else:\n",
    "                            init = concept_drifts[j-2]\n",
    "\n",
    "                        model_j = pickle.load(open(f'./ckpt/CVDTE_{j-1}_{s}', 'rb'))\n",
    "\n",
    "                        if j == 1:\n",
    "                            x_cand = x_all_scale[:concept_drifts[j-1]]\n",
    "                            y_cand = y_all[:concept_drifts[j-1]]\n",
    "                        else:\n",
    "                            x_cand = x_all_scale[concept_drifts[j-2]:concept_drifts[j-1]]\n",
    "                            y_cand = y_all[concept_drifts[j-2]:concept_drifts[j-1]]\n",
    "\n",
    "                        y_pred = model.predict(x_cand)\n",
    "                        prev_y_pred = model_j.predict(x_cand)\n",
    "\n",
    "                        for ind in range(len(y_pred)):\n",
    "                            # condition for determining useful data\n",
    "                            if y_pred[ind] == prev_y_pred[ind] and y_pred[ind] == y_cand[ind]:\n",
    "                                useful_sample.append(init+ind)\n",
    "\n",
    "                    x_useful = x_all_scale[useful_sample]\n",
    "                    y_useful = y_all[useful_sample]\n",
    "\n",
    "                    x_train_final = np.concatenate((x_all_scale[useful_sample], x_train))\n",
    "                    y_train_final = np.concatenate((y_all[useful_sample], y_train))\n",
    "\n",
    "                    # train model with new data and previous useful data (FN+)\n",
    "                    model_fnp = DecisionTreeClassifier(random_state=s)\n",
    "                    model_fnp.fit(x_train_final, y_train_final)\n",
    "\n",
    "                    # find previous best model (FO)\n",
    "                    best_score = 0\n",
    "\n",
    "                    for j in range(1, n+1):\n",
    "                        model_j = pickle.load(open(f'./ckpt/CVDTE_{j-1}_{s}', 'rb'))\n",
    "                        score = model_j.score(x_valid, y_valid)\n",
    "                        if score > best_score:\n",
    "                            model_fo = model_j \n",
    "\n",
    "                    # update previous best model with new data (FO+)\n",
    "                    model_fop = copy.deepcopy(model_fo)\n",
    "                    model_fop.fit(x_train, y_train) \n",
    "\n",
    "                    # compare four different models(FN, FN+, FO, FO+) and choose the best model\n",
    "                    fn_score = model_fn.score(x_valid, y_valid)\n",
    "                    fnp_score = model_fnp.score(x_valid, y_valid)\n",
    "                    fo_score = model_fo.score(x_valid, y_valid)\n",
    "                    fop_score = model_fop.score(x_valid, y_valid)\n",
    "\n",
    "                    if max(fn_score, fnp_score, fo_score, fop_score) == fn_score:\n",
    "                        model = model_fn\n",
    "\n",
    "                    elif max(fn_score, fnp_score, fo_score, fop_score) == fnp_score:\n",
    "                        model = model_fnp\n",
    "\n",
    "                    elif max(fn_score, fnp_score, fo_score, fop_score) == fo_score:\n",
    "                        model = model_fo\n",
    "\n",
    "                    elif max(fn_score, fnp_score, fo_score, fop_score) == fop_score:\n",
    "                        model = model_fop\n",
    "\n",
    "                    train_time = time.time() - start\n",
    "                    train_time_li.append(train_time)\n",
    "\n",
    "                    # final model evaluation\n",
    "                    y_pred = model.predict(x_test)\n",
    "                    y_truth = y_test\n",
    "                    \n",
    "                    test_acc = accuracy_score(y_truth, y_pred)\n",
    "                    if n_class > 2:\n",
    "                        test_f1 = f1_score(y_truth, y_pred, average='weighted')\n",
    "                    elif n_class == 2:\n",
    "                        test_f1 = f1_score(y_truth, y_pred)\n",
    "                    test_acc_li.append(test_acc)\n",
    "                    test_f1_li.append(test_f1)\n",
    "\n",
    "                    pickle.dump(model, open(f'./ckpt/CVDTE_{n}_{s}', 'wb'))\n",
    "                    \n",
    "                all_time.append(np.mean(train_time_li))\n",
    "                all_acc.append(np.mean(test_acc_li))\n",
    "                all_acc_std.append(np.std(test_acc_li))\n",
    "                all_f1.append(np.mean(test_f1_li))\n",
    "                all_f1_std.append(np.std(test_f1_li))\n",
    "\n",
    "        # print runtime, accuracy, and F1 score\n",
    "        print('overall train time: %.3f' %(np.mean(all_time)))\n",
    "        print('overall test acc: avg %.3f, std %.3f' %(np.mean(all_acc), np.mean(all_acc_std)))\n",
    "        print('overall test f1: avg %.3f, std %.3f' %(np.mean(all_f1), np.mean(all_f1_std)))\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data subset selection: GLISTER and GRAD-MATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss functions in model training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion_nored = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Glister_cv(budget):\n",
    "    \"\"\"Glister cross validation.\n",
    "    \n",
    "    Args:\n",
    "        budget: fraction of subset size.\n",
    "    Returns:\n",
    "        Minus value of minimum validation loss.\n",
    "    \"\"\"\n",
    "    # initialize model\n",
    "    model = TwoLayerNet(n_feature, n_class, n_hidden, s)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # set model optimizer, scheduler, and earlystop path\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "    earlystop_path=f'./ckpt/{dataset}_Glister.pt'\n",
    "\n",
    "    # set dataloader parameters\n",
    "    dss_args = dict(model=model,\n",
    "                    loss=criterion_nored,\n",
    "                    eta=0.001,\n",
    "                    num_classes=n_class,\n",
    "                    num_epochs=2000,\n",
    "                    device=device,\n",
    "                    fraction=budget,\n",
    "                    init_budget=split1,\n",
    "                    select_every=20,\n",
    "                    kappa=0,\n",
    "                    linear_layer=True,\n",
    "                    selection_type='PerSample',\n",
    "                    groups=group,\n",
    "                    x_all=x_all_scale,\n",
    "                    y_all=y_all,\n",
    "                   )\n",
    "    dss_args = DotMap(dss_args)\n",
    "\n",
    "    # define GLISTER dataloader\n",
    "    dataloader = GLISTERDataLoader(trainloader, valloader, dss_args, batch_size=128, \n",
    "                                   shuffle=True, pin_memory=False)\n",
    "    \n",
    "    val_losses = list()\n",
    "    val_acc = list()\n",
    "    early_stopping = EarlyStopping.EarlyStopping(patience=10, delta=0.0001, path=earlystop_path)\n",
    "    \n",
    "    # model training with data segment selection\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for k, (inputs, targets, weights) in enumerate(dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            weights = weights.to(device)  \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, last=False, freeze=False)\n",
    "            losses = criterion_nored(outputs, targets)\n",
    "            loss = torch.dot(losses, weights/(weights.sum()))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        # model evaluation using validation set\n",
    "        with torch.no_grad():\n",
    "            for idx, (inputs, targets) in enumerate(valloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True, dtype=torch.int64)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()*targets.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "            val_losses.append(val_loss / val_total)\n",
    "            val_acc.append(val_correct / val_total)\n",
    "\n",
    "        scheduler.step(val_loss/val_total)\n",
    "        early_stopping(val_losses[-1], model)\n",
    "            \n",
    "        if early_stopping.early_stop:\n",
    "            break\n",
    "\n",
    "    return -np.min(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradMatch_cv(budget):\n",
    "    \"\"\"GradMatch cross validation.\n",
    "    \n",
    "    Args:\n",
    "        budget: fraction of subset size.\n",
    "    Returns:\n",
    "        Minus value of minimum validation loss.\n",
    "    \"\"\"\n",
    "    # initialize model\n",
    "    model = TwoLayerNet(n_feature, n_class, n_hidden, s)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # set model optimizer, scheduler, and earlystop path\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "    earlystop_path=f'./ckpt/{dataset}_GradMatch.pt'\n",
    "    \n",
    "    if dataset == 'Sine':\n",
    "        eps = 0.5\n",
    "    else:\n",
    "        eps = 0.01\n",
    "\n",
    "    # set dataloader parameters\n",
    "    dss_args = dict(model=model,\n",
    "                    loss=criterion_nored,\n",
    "                    eta=0.001,\n",
    "                    num_classes=n_class,\n",
    "                    num_epochs=2000,\n",
    "                    device=device,\n",
    "                    fraction=budget,\n",
    "                    init_budget=split1,\n",
    "                    select_every=20,\n",
    "                    kappa=0,\n",
    "                    linear_layer=True,\n",
    "                    selection_type='PerBatch',\n",
    "                    valid=True,\n",
    "                    v1=True, \n",
    "                    lam=0.5, \n",
    "                    eps=eps,\n",
    "                    groups=group,\n",
    "                    x_all=x_all_scale,\n",
    "                    y_all=y_all\n",
    "                   )\n",
    "    dss_args = DotMap(dss_args)\n",
    "\n",
    "    # define GradMatch dataloader\n",
    "    dataloader = GradMatchDataLoader(trainloader, valloader, dss_args, \n",
    "                      batch_size=128, \n",
    "                      shuffle=True,\n",
    "                      pin_memory=False)\n",
    "    \n",
    "    val_losses = list()\n",
    "    val_acc = list()\n",
    "    early_stopping = EarlyStopping.EarlyStopping(patience=10, delta=0.0001, path=earlystop_path)\n",
    "    \n",
    "    # model training with data segment selection\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for k, (inputs, targets, weights) in enumerate(dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            weights = weights.to(device)  \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, last=False, freeze=False)\n",
    "            losses = criterion_nored(outputs, targets)\n",
    "            loss = torch.dot(losses, weights/(weights.sum()))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        # model evaluation using validation set\n",
    "        with torch.no_grad():\n",
    "            for idx, (inputs, targets) in enumerate(valloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True, dtype=torch.int64)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()*targets.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "            val_losses.append(val_loss / val_total)\n",
    "            val_acc.append(val_correct / val_total)\n",
    "\n",
    "        scheduler.step(val_loss/val_total)\n",
    "            \n",
    "        early_stopping(val_losses[-1], model)\n",
    "            \n",
    "        if early_stopping.early_stop:\n",
    "            break\n",
    "\n",
    "    return -np.min(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dss(dataloader, trainloader, valloader, testloader, model, optimizer, scheduler, num_epochs, earlystop_path):\n",
    "    \"\"\"Run data segment selection algorithm.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: Dataloader with data subset selection method.\n",
    "        trainloader: Train dataloader.\n",
    "        valloader: Validation dataloader.\n",
    "        testloader: Test dataloader.\n",
    "        model: Train model.\n",
    "        optimizer: Model optimizer.\n",
    "        scheduler: Learning rate scheduler.\n",
    "        num_epochs: Number of maximum epochs.\n",
    "        earlystop_path: Earlystop model checkpoint path.\n",
    "    Returns:\n",
    "        Earlystop epoch, best accuracy, best f1 score, and runtime.\n",
    "    \"\"\"\n",
    "    val_losses = list() \n",
    "    tst_acc = list()\n",
    "    tst_f1 = list()\n",
    "    timing = list()\n",
    "    \n",
    "    # set model optimizer, scheduler, and earlystop path\n",
    "    early_stopping = EarlyStopping.EarlyStopping(patience=10, delta=0.0001, path=earlystop_path)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "    \n",
    "    # model training with data segment selection\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        start_time = time.time()   \n",
    "        \n",
    "        for k, (inputs, targets, weights) in enumerate(dataloader):\n",
    "            train_start = time.time()\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            weights = weights.to(device)  \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, last=False, freeze=False)\n",
    "            losses = criterion_nored(outputs, targets)\n",
    "            loss = torch.dot(losses, weights/(weights.sum()))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        epoch_time = time.time() - start_time\n",
    "        timing.append(epoch_time)\n",
    "\n",
    "        val_loss = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        # model evaluation using validation set\n",
    "        with torch.no_grad():\n",
    "            for idx, (inputs, targets) in enumerate(valloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True, dtype=torch.int64)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()*targets.size(0)\n",
    "                val_total += targets.size(0)\n",
    "            val_losses.append(val_loss / val_total)\n",
    "\n",
    "        scheduler.step(val_loss/val_total)\n",
    "        \n",
    "        y_pred = []\n",
    "        y_truth = []\n",
    "\n",
    "        # model evaluation using test set\n",
    "        with torch.no_grad():\n",
    "            for _, (inputs, targets) in enumerate(testloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True, dtype=torch.int64)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                y_pred.append(predicted.cpu().detach().numpy())\n",
    "                y_truth.append(targets.cpu().detach().numpy())\n",
    "\n",
    "        y_pred = [item for sublist in y_pred for item in sublist]\n",
    "        y_truth = [item for sublist in y_truth for item in sublist]\n",
    "\n",
    "        tst_acc.append(accuracy_score(y_truth, y_pred))\n",
    "        \n",
    "        if n_class > 2:\n",
    "            tst_f1.append(f1_score(y_truth, y_pred, average='weighted'))\n",
    "        elif n_class == 2:\n",
    "            tst_f1.append(f1_score(y_truth, y_pred))\n",
    "            \n",
    "        early_stopping(val_losses[-1], model)\n",
    "            \n",
    "        if early_stopping.early_stop:\n",
    "            break\n",
    "\n",
    "    timing_array = np.array(timing)\n",
    "    best_ind = np.argmin(val_losses)\n",
    "    \n",
    "    tmp = 0\n",
    "    mod_cum_timing = np.zeros(len(timing_array))\n",
    "    for i in range(len(timing_array)):\n",
    "        tmp += timing_array[i]\n",
    "        mod_cum_timing[i] = tmp\n",
    "    cum_timing = list(mod_cum_timing)\n",
    "    \n",
    "    return best_ind, tst_acc[best_ind], tst_f1[best_ind], cum_timing[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:  SEA\n",
      "concept drifts:  [ 2000  4000  6000  8000 10000 12000 14000 16000]\n",
      "----------------------------------------------------------------------------\n",
      "method:  Glister\n",
      "overall train time: 23.613\n",
      "overall test acc: avg 0.856, std 0.009\n",
      "overall test f1: avg 0.885, std 0.008\n",
      "----------------------------------------------------------------------------\n",
      "method:  GradMatch\n",
      "overall train time: 2.847\n",
      "overall test acc: avg 0.853, std 0.007\n",
      "overall test f1: avg 0.884, std 0.005\n",
      "\n",
      "\n",
      "dataset:  Hyperplane\n",
      "concept drifts:  [ 2000  4000  6000  8000 10000 12000 14000 16000]\n",
      "----------------------------------------------------------------------------\n",
      "method:  Glister\n",
      "overall train time: 22.704\n",
      "overall test acc: avg 0.907, std 0.006\n",
      "overall test f1: avg 0.907, std 0.006\n",
      "----------------------------------------------------------------------------\n",
      "method:  GradMatch\n",
      "overall train time: 1.257\n",
      "overall test acc: avg 0.841, std 0.007\n",
      "overall test f1: avg 0.843, std 0.008\n",
      "\n",
      "\n",
      "dataset:  RandomRBF\n",
      "concept drifts:  [ 2000  4000  6000  8000 10000 12000 14000 16000]\n",
      "----------------------------------------------------------------------------\n",
      "method:  Glister\n",
      "overall train time: 65.387\n",
      "overall test acc: avg 0.794, std 0.016\n",
      "overall test f1: avg 0.794, std 0.015\n",
      "----------------------------------------------------------------------------\n",
      "method:  GradMatch\n",
      "overall train time: 7.113\n",
      "overall test acc: avg 0.788, std 0.020\n",
      "overall test f1: avg 0.788, std 0.020\n",
      "\n",
      "\n",
      "dataset:  Sine\n",
      "concept drifts:  [ 2000  4000  6000  8000 10000 12000 14000 16000]\n",
      "----------------------------------------------------------------------------\n",
      "method:  Glister\n",
      "overall train time: 15.199\n",
      "overall test acc: avg 0.876, std 0.012\n",
      "overall test f1: avg 0.873, std 0.016\n",
      "----------------------------------------------------------------------------\n",
      "method:  GradMatch\n",
      "overall train time: 0.922\n",
      "overall test acc: avg 0.535, std 0.071\n",
      "overall test f1: avg 0.534, std 0.092\n",
      "\n",
      "\n",
      "dataset:  Electricity\n",
      "concept drifts:  [ 4320  8640 12960 17280 21600 25920 30240 34560 38880 43200]\n",
      "----------------------------------------------------------------------------\n",
      "method:  Glister\n",
      "overall train time: 88.039\n",
      "overall test acc: avg 0.699, std 0.016\n",
      "overall test f1: avg 0.743, std 0.023\n",
      "----------------------------------------------------------------------------\n",
      "method:  GradMatch\n",
      "overall train time: 6.923\n",
      "overall test acc: avg 0.689, std 0.013\n",
      "overall test f1: avg 0.752, std 0.013\n",
      "\n",
      "\n",
      "dataset:  Weather\n",
      "concept drifts:  [ 1800  3600  5400  7200  9000 10800 12600 14400 16200 18000]\n",
      "----------------------------------------------------------------------------\n",
      "method:  Glister\n",
      "overall train time: 46.932\n",
      "overall test acc: avg 0.795, std 0.009\n",
      "overall test f1: avg 0.653, std 0.020\n",
      "----------------------------------------------------------------------------\n",
      "method:  GradMatch\n",
      "overall train time: 3.840\n",
      "overall test acc: avg 0.796, std 0.005\n",
      "overall test f1: avg 0.625, std 0.014\n",
      "\n",
      "\n",
      "dataset:  Spam\n",
      "concept drifts:  [1036 2072 3108 4144 5180 6216 7252 8288 9324]\n",
      "----------------------------------------------------------------------------\n",
      "method:  Glister\n",
      "overall train time: 14.167\n",
      "overall test acc: avg 0.971, std 0.004\n",
      "overall test f1: avg 0.974, std 0.003\n",
      "----------------------------------------------------------------------------\n",
      "method:  GradMatch\n",
      "overall train time: 1.244\n",
      "overall test acc: avg 0.969, std 0.006\n",
      "overall test f1: avg 0.972, std 0.004\n",
      "\n",
      "\n",
      "dataset:  Usenet1\n",
      "concept drifts:  [ 300  600  900 1200 1500]\n",
      "----------------------------------------------------------------------------\n",
      "method:  Glister\n",
      "overall train time: 2.448\n",
      "overall test acc: avg 0.770, std 0.020\n",
      "overall test f1: avg 0.737, std 0.032\n",
      "----------------------------------------------------------------------------\n",
      "method:  GradMatch\n",
      "overall train time: 0.166\n",
      "overall test acc: avg 0.609, std 0.085\n",
      "overall test f1: avg 0.569, std 0.115\n",
      "\n",
      "\n",
      "dataset:  Usenet2\n",
      "concept drifts:  [ 300  600  900 1200 1500]\n",
      "----------------------------------------------------------------------------\n",
      "method:  Glister\n",
      "overall train time: 2.244\n",
      "overall test acc: avg 0.746, std 0.029\n",
      "overall test f1: avg 0.603, std 0.042\n",
      "----------------------------------------------------------------------------\n",
      "method:  GradMatch\n",
      "overall train time: 0.145\n",
      "overall test acc: avg 0.686, std 0.028\n",
      "overall test f1: avg 0.435, std 0.084\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_li = ['SEA', 'Hyperplane', 'RandomRBF', 'Sine', 'Electricity', 'Weather', 'Spam', 'Usenet1', 'Usenet2']\n",
    "\n",
    "for dataset in dataset_li:\n",
    "    # load data, label, and concept drift points\n",
    "    x_all = np.load(f'./dataset/{dataset}/data.npy')\n",
    "    y_all = np.load(f'./dataset/{dataset}/label.npy')\n",
    "    concept_drifts = np.load(f'./dataset/{dataset}/concept_drifts.npy')\n",
    "    \n",
    "    # number of classes in dataset\n",
    "    if dataset == 'RandomRBF':\n",
    "        n_class = 5\n",
    "    else:\n",
    "        n_class = 2\n",
    "    \n",
    "    # number of nodes in hidden layer\n",
    "    n_hidden = 256\n",
    "    \n",
    "    # number of maximum epochs\n",
    "    num_epochs=2000\n",
    "    \n",
    "    # number of available data in current segment\n",
    "    if dataset in ['Spam', 'Usenet1', 'Usenet2']:\n",
    "        n_train = int((len(x_all)/len(concept_drifts))*0.2)\n",
    "    else:\n",
    "        n_train = int((len(x_all)/len(concept_drifts))*0.1)\n",
    "        \n",
    "    # split available data into train and valid\n",
    "    split1 = int(n_train*0.5)\n",
    "    \n",
    "    print('dataset: ', dataset)\n",
    "    print('concept drifts: ', concept_drifts)\n",
    "\n",
    "    method_li = ['Glister', 'GradMatch']\n",
    "    \n",
    "    for method in method_li:\n",
    "        print(\"----------------------------------------------------------------------------\")\n",
    "        print('method: ', method)\n",
    "    \n",
    "        all_time = []\n",
    "        all_acc = []\n",
    "        all_acc_std = []\n",
    "        all_f1 = []\n",
    "        all_f1_std = []\n",
    "\n",
    "        # consecutive evaluation with data segments\n",
    "        for n in range(len(concept_drifts)):\n",
    "            train_time_li = []\n",
    "            test_acc_li = []\n",
    "            test_f1_li = []\n",
    "\n",
    "            n_dataset = n+1\n",
    "            n_feature = x_all.shape[1]\n",
    "\n",
    "            if n == 0:\n",
    "                num = concept_drifts[n]\n",
    "            else:\n",
    "                num = concept_drifts[n]-concept_drifts[n-1]\n",
    "\n",
    "            # data preprocessing (scaling)\n",
    "            scaler = StandardScaler()\n",
    "            x_all_scale = scaler.fit_transform(x_all)\n",
    "\n",
    "            # repeat experiments with 5 different seeds\n",
    "            for s in range(5):\n",
    "                # initialize model\n",
    "                model = TwoLayerNet(n_feature, n_class, n_hidden, s)\n",
    "                model = model.to(device)\n",
    "                \n",
    "                # prepare data for train, valid, and test\n",
    "                dataset_all = range(n_dataset)\n",
    "                train_ds, valid_ds, test_ds, train_index = prepare_data(n, n_train, x_all_scale, y_all, \n",
    "                                                                        concept_drifts, dataset_all, n_feature, \n",
    "                                                                        device)\n",
    "                \n",
    "                trainloader = DataLoader(train_ds, batch_size=128, shuffle=False)\n",
    "                valloader = DataLoader(valid_ds, batch_size=128, shuffle=True)\n",
    "                testloader = DataLoader(test_ds, batch_size=128, shuffle=True)\n",
    "            \n",
    "                if method == 'Glister':\n",
    "                    # split data into sample unit\n",
    "                    if n == 0:\n",
    "                        arr = np.arange(split1)\n",
    "                        group = np.split(arr, len(arr))\n",
    "                    else:\n",
    "                        arr = np.arange(n*int((len(x_all)/len(concept_drifts)))+split1)\n",
    "                        group = np.split(arr, len(arr))\n",
    "                    \n",
    "                    # cross validation to find best budget value\n",
    "                    pbounds = [0.1*b for b in range(1,11)]\n",
    "                    glister_perf = []\n",
    "                    for budget in pbounds:\n",
    "                        perf = Glister_cv(budget)\n",
    "                        glister_perf.append(perf)\n",
    "                    best_budget = pbounds[np.argmax(glister_perf)]\n",
    "\n",
    "                    # initialize model\n",
    "                    model = TwoLayerNet(n_feature, n_class, n_hidden, s)\n",
    "                    model = model.to(device)\n",
    "\n",
    "                    # set model optimizer, scheduler, and earlystop path\n",
    "                    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "                    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "                    earlystop_path=f'./ckpt/{dataset}_Glister.pt'\n",
    "\n",
    "                    # set arguments for data subset selection\n",
    "                    dss_args = dict(model=model,\n",
    "                                    loss=criterion_nored,\n",
    "                                    eta=0.001,\n",
    "                                    num_classes=n_class,\n",
    "                                    num_epochs=2000,\n",
    "                                    device=device,\n",
    "                                    fraction=best_budget,\n",
    "                                    init_budget=split1,\n",
    "                                    select_every=20,\n",
    "                                    kappa=0,\n",
    "                                    linear_layer=True,\n",
    "                                    selection_type='PerSample',\n",
    "                                    groups=group,\n",
    "                                    x_all=x_all_scale,\n",
    "                                    y_all=y_all\n",
    "                                   )\n",
    "                    dss_args = DotMap(dss_args)\n",
    "\n",
    "                    # define GLISTER dataloader\n",
    "                    dataloader = GLISTERDataLoader(trainloader, valloader, dss_args, batch_size=128, \n",
    "                                                   shuffle=True, pin_memory=False)\n",
    "\n",
    "                    # model training and evaluation\n",
    "                    best_ind, best_acc, best_f1, runtime = run_dss(dataloader, trainloader, valloader, testloader, \n",
    "                                                                   model, optimizer, scheduler, num_epochs, \n",
    "                                                                   earlystop_path=f'./ckpt/{dataset}_Glister.pt')\n",
    "                \n",
    "                elif method == 'GradMatch':\n",
    "                    # split data into random batch unit\n",
    "                    if n == 0:\n",
    "                        group = [np.arange(split1)]\n",
    "                    else:\n",
    "                        group = []\n",
    "                        arr = np.arange(n*int((len(x_all)/len(concept_drifts)))+split1)\n",
    "                        np.random.seed(s)\n",
    "                        np.random.shuffle(arr)\n",
    "                        batch_size = 128\n",
    "                        num_batch = math.ceil(len(arr)/batch_size)\n",
    "                            \n",
    "                        for i in range(num_batch):\n",
    "                            if i == num_batch-1:\n",
    "                                group.append(arr[i*batch_size:])\n",
    "                            else:\n",
    "                                group.append(arr[i*batch_size:(i+1)*batch_size])\n",
    "                                \n",
    "                    # cross validation to find best budget value\n",
    "                    pbounds = [0.1*b for b in range(1,11)]\n",
    "                    gradmatch_perf = []\n",
    "                    for budget in pbounds:\n",
    "                        perf = GradMatch_cv(budget)\n",
    "                        gradmatch_perf.append(perf)\n",
    "                    best_budget = pbounds[np.argmax(gradmatch_perf)]\n",
    "\n",
    "                    # initialize model\n",
    "                    model = TwoLayerNet(n_feature, n_class, n_hidden, s)\n",
    "                    model = model.to(device)\n",
    "\n",
    "                    # set model optimizer, scheduler, and earlystop path\n",
    "                    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "                    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "                    earlystop_path=f'./ckpt/{dataset}_GradMatch.pt'\n",
    "                    \n",
    "                    if dataset == 'Sine':\n",
    "                        eps = 0.5\n",
    "                    else:\n",
    "                        eps = 0.01\n",
    "\n",
    "                    # set arguments for data subset selection\n",
    "                    dss_args = dict(model=model,\n",
    "                                    loss=criterion_nored,\n",
    "                                    eta=0.001,\n",
    "                                    num_classes=n_class,\n",
    "                                    num_epochs=2000,\n",
    "                                    device=device,\n",
    "                                    fraction=best_budget,\n",
    "                                    init_budget=split1,\n",
    "                                    select_every=20,\n",
    "                                    kappa=0,\n",
    "                                    linear_layer=True,\n",
    "                                    selection_type='PerBatch',\n",
    "                                    valid=True,\n",
    "                                    v1=True, \n",
    "                                    lam=0.5, \n",
    "                                    eps=eps,\n",
    "                                    groups=group,\n",
    "                                    x_all=x_all_scale,\n",
    "                                    y_all=y_all\n",
    "                                   )\n",
    "                    dss_args = DotMap(dss_args)\n",
    "                    \n",
    "                    # define GradMatch dataloader\n",
    "                    dataloader = GradMatchDataLoader(trainloader, valloader, dss_args, batch_size=128, \n",
    "                                                     shuffle=True, pin_memory=False)\n",
    "\n",
    "                    # model training and evaluation\n",
    "                    best_ind, best_acc, best_f1, runtime = run_dss(dataloader, trainloader, valloader, testloader, \n",
    "                                                                   model, optimizer, scheduler, num_epochs, \n",
    "                                                                   earlystop_path=f'./ckpt/{dataset}_GradMatch.pt')\n",
    "\n",
    "                train_time_li.append(runtime)\n",
    "                test_acc_li.append(best_acc)\n",
    "                test_f1_li.append(best_f1)\n",
    "\n",
    "            all_time.append(np.mean(train_time_li))\n",
    "            all_acc.append(np.mean(test_acc_li))\n",
    "            all_acc_std.append(np.std(test_acc_li))\n",
    "            all_f1.append(np.mean(test_f1_li))\n",
    "            all_f1_std.append(np.std(test_f1_li))\n",
    "\n",
    "        # print runtime, accuracy, and F1 score\n",
    "        print('overall train time: %.3f' %(np.mean(all_time)))\n",
    "        print('overall test acc: avg %.3f, std %.3f' %(np.mean(all_acc), np.mean(all_acc_std)))\n",
    "        print('overall test f1: avg %.3f, std %.3f' %(np.mean(all_f1), np.mean(all_f1_std)))\n",
    "\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
